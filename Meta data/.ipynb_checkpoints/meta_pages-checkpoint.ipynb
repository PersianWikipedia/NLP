{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8  -*-\n",
    "#\n",
    "# Reza(User:reza1615), 2019\n",
    "# Distributed under the terms of MIT License (MIT)\n",
    "#\n",
    "import sys\n",
    "sys.path.insert(0, r\"G:\\Outwork\\wikipedia\\pycore\")\n",
    "\n",
    "from pywikibot import config\n",
    "from ip2geotools.databases.noncommercial import DbIpCity\n",
    "from diff_match_patch import diff_match_patch\n",
    "import re\n",
    "import json\n",
    "import pywikibot\n",
    "import MySQLdb\n",
    "import pandas\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "fasite = pywikibot.Site('fa', 'wikipedia')\n",
    "site = pywikibot.Site('fa', 'wikipedia')\n",
    "_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revision_info (revid):\n",
    "    if _cache.get(tuple([revid, fasite, 'Revision_info'])):\n",
    "        return _cache[tuple([revid, fasite, 'Revision_info'])]\n",
    "    #https://fa.wikipedia.org/w/api.php?action=query&prop=revisions&revids=347819&rvprop=user|ids|tags|comment|timestamp|size\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'prop': 'revisions',\n",
    "        'revids': revid,\n",
    "        'rvprop': 'user|ids|tags|comment|timestamp|size'\n",
    "    }\n",
    "    query = pywikibot.data.api.Request(site=fasite, **params).submit()\n",
    "    pagedict=query[u'query'][u'pages']\n",
    "    pagedict=pagedict[list(pagedict.keys())[0]]\n",
    "    page_id=pagedict['pageid']\n",
    "    page_title=pagedict[\"title\"]\n",
    "    user=pagedict[\"revisions\"][0]['user']\n",
    "    timestamp=pagedict[\"revisions\"][0]['timestamp']\n",
    "    info_dict={revid:{'title':page_title,'id':page_id,'user':user,'time':timestamp}}\n",
    "    \n",
    "    _cache[tuple([revid, fasite, 'Revision_info'])]=info_dict\n",
    "    return info_dict\n",
    "\n",
    "def compare_texts (text1,text2):\n",
    "    dmp = diff_match_patch()\n",
    "    patches = dmp.patch_make(text1,text2)\n",
    "    diff = dmp.patch_toText(patches)\n",
    "    return diff\n",
    "\n",
    "def compare_revisions(id1,id2):\n",
    "    #https://fa.wikipedia.org/w/api.php?action=compare&fromrev=24275599&torev=26278079\n",
    "    params = {\n",
    "        'action': 'compare',\n",
    "        'fromrev': id1,\n",
    "        'torev': id2,\n",
    "    }\n",
    "    query = pywikibot.data.api.Request(site=fasite, **params).submit()\n",
    "    Compare_txt=query[u'compare'][u'*']\n",
    "    soup = BeautifulSoup(Compare_txt)\n",
    "    added_list = soup.findAll(\"td\", {\"class\": \"diff-addedline\"})\n",
    "    deleted_list = soup.findAll(\"td\", {\"class\": \"diff-deletedline\"})\n",
    "\n",
    "    return added_list,deleted_list\n",
    "\n",
    "def get_user_R_G (UserName):\n",
    "    if _cache.get(tuple([UserName, fasite, 'user'])):\n",
    "        return _cache[tuple([UserName, fasite, 'user'])]\n",
    "    UserName = UserName.replace(u' ', u'_')\n",
    "    #https://fa.wikipedia.org/w/api.php?action=query&list=users&ususers=Ahmad252&usprop=groups|rights|editcount|registration\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'list': 'users',\n",
    "        'ususers': UserName,\n",
    "        'usprop': 'groups|rights|editcount|registration',\n",
    "    }\n",
    "    query = pywikibot.data.api.Request(site=fasite, **params).submit()\n",
    "    user_rights=query[u'query'][u'users'][0][\"rights\"]\n",
    "    user_Groups=query[u'query'][u'users'][0][\"groups\"]\n",
    "    user_editcount=query[u'query'][u'users'][0][\"editcount\"]\n",
    "    user_registration_year=query[u'query'][u'users'][0][\"registration\"].split('-')[0]\n",
    "    \n",
    "    _cache[tuple([UserName, fasite, 'user'])] = user_rights,user_Groups,user_editcount,user_registration_year\n",
    "    return user_rights,user_Groups,user_editcount,user_registration_year\n",
    "\n",
    "def Page_veiw (title):\n",
    "    if _cache.get(tuple([title, fasite, 'Page_veiw'])):\n",
    "        return _cache[tuple([title, fasite, 'Page_veiw'])]\n",
    "    title = title.replace(u' ', u'_')\n",
    "    #https://fa.wikipedia.org/w/api.php?action=query&titles=%D8%A7%DB%8C%D8%B1%D8%A7%D9%86&prop=pageviews\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'titles': 'title',\n",
    "        'prop': 'pageviews',\n",
    "    }\n",
    "    query = pywikibot.data.api.Request(site=fasite, **params).submit()\n",
    "    dict=query[u'query'][u'pages']\n",
    "    page_views=dict[list(dict.keys())[0]][\"pageviews\"]\n",
    "    Page_last_month_view=sum([page_views[i] for i in page_views][-30:])\n",
    "    _cache[tuple([title, fasite, 'Page_veiw'])]=Page_last_month_view\n",
    "    return Page_last_month_view\n",
    "\n",
    "def watchers (title):\n",
    "    if _cache.get(tuple([title, fasite, 'watchers'])):\n",
    "        return _cache[tuple([title, fasite, 'watchers'])]\n",
    "    title = title.replace(u' ', u'_')\n",
    "    #https://fa.wikipedia.org/w/api.php?action=query&prop=info&inprop=visitingwatchers&titles=%D8%A7%DB%8C%D8%B1%D8%A7%D9%86\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'titles': title,\n",
    "        'prop': 'info',\n",
    "        'inprop': 'watchers|visitingwatchers'\n",
    "    }\n",
    "    query = pywikibot.data.api.Request(site=fasite, **params).submit()\n",
    "    dict=query[u'query'][u'pages']\n",
    "    visitingwatchers=dict[list(dict.keys())[0]][\"visitingwatchers\"]\n",
    "    watchers=dict[list(dict.keys())[0]][\"watchers\"]\n",
    "    _cache[tuple([title, fasite, 'Page_veiw'])] = visitingwatchers,watchers\n",
    "    return visitingwatchers,watchers\n",
    "\n",
    "def revsion_txt (title, rvstart):\n",
    "    title = title.replace(u' ', u'_')\n",
    "    #https://fa.wikipedia.org/w/api.php?action=query&prop=revisions&titles=چنگال&rvlimit=10&rvslots=main&rvprop=content&rvdir=newer&rvstart=2016-07-01T00:00:00Z&rvexcludeuser=SSethiایران\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'titles': title,\n",
    "        'prop': 'revisions',\n",
    "        'rvlimit':1,\n",
    "        'rvslots':'main',\n",
    "        'rvprop':'content|flags|ids|timestamp|user|comment|tags|size',\n",
    "        'rvdir':'newer',\n",
    "        'rvstart':rvstart\n",
    "    }\n",
    "    query = pywikibot.data.api.Request(site=fasite, **params).submit()\n",
    "    revision=query[u'query'][u'pages'][list(dict.keys())[0]][\"pageid\"]['revisions'][0]\n",
    "    flags=revision [\"flags\"]\n",
    "    revids=revision [\"revid\"]\n",
    "    users=revision [\"user\"]\n",
    "    timestamp=revision [\"timestamp\"]\n",
    "    size=revision [\"size\"]\n",
    "    comment=revision [\"comment\"]\n",
    "    tags=revision [\"tags\"]\n",
    "    txt=revision [\"content\"]\n",
    "    return [flags,revids,users,timestamp,size,comment,tags ,txt]\n",
    "\n",
    "def history_log (title,year,RevID):\n",
    "    title = title.replace(u' ', u'_')\n",
    "    #https://fa.wikipedia.org/w/api.php?action=query&prop=revisions&titles=چنگال&rvlimit=10&rvslots=main&rvprop=timestamp|user|comment|tags|size|ids&rvdir=newer&rvstart=2016-07-01T00:00:00Z&rvexcludeuser=SSethiایران\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'titles': title,\n",
    "        'prop': 'revisions',\n",
    "        'rvlimit':500,\n",
    "        'rvslots':'main',\n",
    "        'rvprop':'flags|ids|timestamp|user|comment|tags|size',\n",
    "        'rvdir':'newer',\n",
    "        'rvstart':str(year-3)+'-01-01T00:00:00Z',\n",
    "        'rvexcludeuser':'Ali'\n",
    "    }\n",
    "    query = pywikibot.data.api.Request(site=fasite, **params).submit()\n",
    "    dict=query[u'query'][u'pages']\n",
    "    dict=dict[list(dict.keys())[0]]\n",
    "    pageid=dict[\"pageid\"]\n",
    "    revisions=dict[\"pageid\"]['revisions']\n",
    "    flags,revids,users,timestamp,size,comment,tags,current=[],[],[],[],[],[],[],[]\n",
    "    for i in revisions:\n",
    "        if RevID == i[\"revid\"]:\n",
    "            current = revsion_txt (title, i[\"timestamp\"])\n",
    "        flags.append(i[\"flags\"])\n",
    "        revids.append(i[\"revid\"])\n",
    "        users.append(i[\"user\"])\n",
    "        timestamp.append(i[\"timestamp\"])\n",
    "        size.append(i[\"size\"])\n",
    "        comment.append(i[\"comment\"])\n",
    "        tags.append(i[\"tags\"])\n",
    "    return pageid,flags,revids,users,timestamp,size,comment,tags ,current\n",
    "\n",
    "def get_ip_city(IP):\n",
    "    if _cache.get(tuple([IP, fasite, 'ip_city'])):\n",
    "        return _cache[tuple([IP, fasite, 'ip_city'])]\n",
    "    #ip='147.229.2.90'\n",
    "    response = DbIpCity.get(IP, api_key='free')\n",
    "    response=json.loads(response.to_json())\n",
    "    #'{\"ip_address\": \"5.121.94.147\", \"city\": \"Tehr\\\\u0101n\", \"region\": \"Tehr\\\\u0101n\", \"country\": \"IR\", \"latitude\": 35.7006177, \"longitude\": 51.4013785}'\n",
    "    _cache[tuple([IP, fasite, 'ip_city'])] = response['city']+', '+response['country']\n",
    "    return  response['city']+', '+response['country']\n",
    "\n",
    "def sql_revert_by_user (user):\n",
    "    user = user.replace(u' ', u'_')\n",
    "    if _cache.get(tuple([user, fasite, 'revert_by_user'])):\n",
    "        return _cache[tuple([user, fasite, 'revert_by_user'])]\n",
    "        \n",
    "    query = \"\"\"SELECT count (*)\n",
    "                FROM (select rev_id,rev_page,rev_actor,rev_comment_id from revision where rev_timestamp > NOW() - INTERVAL 6 MONTH) R \n",
    "                JOIN page ON R.rev_page=page_id AND page_namespace = 0\n",
    "                JOIN actor ON R.rev_actor = actor_id and actor_name LIKE '\"\"\"+user +\"\"\"'\n",
    "                JOIN comment ON R.rev_comment_id = comment_id and   \n",
    "                comment_text LIKE '%خنثی‌سازی%' and comment_text NOT LIKE '%\"\"\"+user +\"\"\"%'\n",
    "                group by R.rev_id\"\"\"\n",
    "                \n",
    "    cn = MySQLdb.connect(\"fawiki.labsdb\", db=site.dbName()+ '_p', user=config.db_username, passwd=config.db_password)\n",
    "    cur = cn.cursor()\n",
    "    cur.execute(query)\n",
    "    result = cur.fetchall()\n",
    "    cn.close()\n",
    "    _cache[tuple([user, fasite, 'revert_by_user'])] = len(result)\n",
    "    return len(result)\n",
    "\n",
    "def sql_revert_by_other (user):\n",
    "    user = user.replace(u' ', u'_')\n",
    "    if _cache.get(tuple([user, fasite, 'revert_by_other'])):\n",
    "        return _cache[tuple([user, fasite, 'revert_by_other'])]\n",
    "        \n",
    "    query = \"\"\"SELECT count (*)\n",
    "                FROM (select rev_id,rev_page,rev_actor,rev_comment_id from revision where rev_timestamp > NOW() - INTERVAL 6 MONTH) R \n",
    "                JOIN page ON R.rev_page=page_id AND page_namespace = 0\n",
    "                JOIN actor ON R.rev_actor = actor_id and actor_name NOT LIKE '\"\"\"+user +\"\"\"'\n",
    "                JOIN comment ON R.rev_comment_id = comment_id and   \n",
    "                comment_text LIKE '%خنثی‌سازی%' and comment_text LIKE '%\"\"\"+user +\"\"\"%'\n",
    "                group by R.rev_id\"\"\"\n",
    "    cn = MySQLdb.connect(\"fawiki.labsdb\", db=site.dbName()+ '_p', user=config.db_username, passwd=config.db_password)\n",
    "    cur = cn.cursor()\n",
    "    cur.execute(query)\n",
    "    result = cur.fetchall()\n",
    "    cn.close()\n",
    "    _cache[tuple([user, fasite, 'revert_by_other'])] = len(result)\n",
    "    return len(result)\n",
    "\n",
    "def sql_blocked_users ():\n",
    "    query = \"\"\" SELECT log_title, COUNT(*)\n",
    "                FROM logging\n",
    "                WHERE log_type = \"block\"\n",
    "                    AND log_timestamp > NOW() - INTERVAL 1 YEAR\n",
    "                    AND REGEXP_REPLACE(log_title, '[0-9\\.]+', '') <> \"\"\n",
    "                GROUP BY log_title\n",
    "                ORDER BY COUNT(*) DESC;\"\"\"\n",
    "    cn = MySQLdb.connect(\"fawiki.labsdb\", db=site.dbName()+ '_p', user=config.db_username, passwd=config.db_password)\n",
    "    cur = cn.cursor()\n",
    "    cur.execute(query)\n",
    "    BlockedUsers = cur.fetchall()\n",
    "    cn.close()\n",
    "    return BlockedUsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links():\n",
    "    project_page='ویکی‌پدیا:واحد ضد خرابکاری/ربات/ارزشیابی'\n",
    "    extenstions=['','/بایگانی ۲','/بایگانی ۱']\n",
    "    fa_text=''\n",
    "    for extend in extenstions:\n",
    "        page_link=project_page+extend\n",
    "        fapage = pywikibot.Page(fasite, page_link)\n",
    "        fa_text += fapage.get().strip()+'\\n'\n",
    "    return fa_text.replace('\\r','').split('\\n')\n",
    "\n",
    "def filter_links(links):\n",
    "    link_dict={}\n",
    "    for link in links:\n",
    "        if not '#' in link or not '|' in link:\n",
    "            continue\n",
    "        diff_id=link.split('|')[1]\n",
    "        tag=link.split('|')[2].strip().split('-')\n",
    "        #user=link.split('|')[3]\n",
    "        if diff_id in link_dict:\n",
    "            link_dict[diff_id]=list(set(link_dict[diff_id]+tag))\n",
    "        else:\n",
    "            link_dict[diff_id]=tag\n",
    "    return link_dict\n",
    "\n",
    "def link_analyzer (link):#---------------\n",
    "    RevID= link.split()\n",
    "    id2= link.split()\n",
    "    UserName= link.split()\n",
    "    title= link.split()\n",
    "    return RevID,id2,UserName,title\n",
    "\n",
    "def Mixer ():\n",
    "    # Mix to each other\n",
    "    # meta data\n",
    "    pd.to_csv('MetaData.csv') #-------- append\n",
    "    \n",
    "def run (rev_id):\n",
    "    year=date.today().year\n",
    "    year='2019' # should be today ()\n",
    "    rev_dict = revision_info (rev_id)\n",
    "    #     {26790691: {\n",
    "    #                 'title': 'رونالدینیو',\n",
    "    #                    'id': 6978,\n",
    "    #                  'user': 'Mmehdih',\n",
    "    #                  'time': '2019-08-14T12:01:13Z'\n",
    "    #                }\n",
    "    #     }\n",
    "    title = rev_dict[rev_id]['title']\n",
    "    user = rev_dict[rev_id]['user']\n",
    "    time = rev_dict[rev_id]['time']\n",
    "    print('get rev_info ....')\n",
    "    print(title,user,time)\n",
    "    \n",
    "    #     diff_dict = compare_texts (text1,text2)\n",
    "    #     Compare_txt = compare_revisions(RevID,id2)\n",
    "    \n",
    "   \n",
    "    #Page_last_month_view = Page_veiw (title)\n",
    "    #print('get Page_veiw ....')\n",
    "    #print(Page_last_month_view)\n",
    "    \n",
    "#     visitingwatchers, thewatchers = watchers (title)\n",
    "#     print('get watchers ....')\n",
    "#     print(visitingwatchers, thewatchers)\n",
    "    \n",
    "#     pageid,flags,revids,users,timestamp,size,comment,tags ,current = history_log (title,year,rev_id)\n",
    "#     print('get history_log ....')\n",
    "#     print(pageid,flags,revids,users,timestamp,size,comment,tags ,current)\n",
    "    \n",
    "#     BlockedUsers_list = sql_blocked_users ()\n",
    "    \n",
    "    # IP and users\n",
    "    if not re.sub('[0-9\\.]+','',user).strip():\n",
    "        the_city= get_ip_city(user)\n",
    "        print('get get_ip_city ....')\n",
    "        print(the_city)\n",
    "    else:\n",
    "        user_rights,user_Groups,user_editcount,user_registration_year = get_user_R_G (user)\n",
    "        print('get get_user_R_G ....')\n",
    "        print(user_rights,user_Groups,user_editcount,user_registration_year)\n",
    "#         len_revert_by= sql_revert_by_user (user)\n",
    "#         print('get sql_revert_by_user ....')\n",
    "#         print(len_revert_by)\n",
    "#         len_reverted= sql_revert_by_other (user)\n",
    "#         print('get sql_revert_by_other ....')\n",
    "#         print(len_reverted)\n",
    "        pass\n",
    "        \n",
    "    # Mixer\n",
    "    #Mixer ()\n",
    "    \n",
    "def main():\n",
    "    links = get_links()\n",
    "    link_dict=filter_links(links)\n",
    "    for link in link_dict:\n",
    "        run (link)\n",
    "        #break\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = get_links()\n",
    "link_dict=filter_links(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link_dict={'26634694': ['Nحذف_نامناسب', 'Nواگردانی']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revision_info (26790691)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old=26790691\n",
    "new=26798425\n",
    "text,b= compare_revisions(old,new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(text)\n",
    "added_list = soup.findAll(\"td\", {\"class\": \"diff-addedline\"})\n",
    "deleted_list = soup.findAll(\"td\", {\"class\": \"diff-deletedline\"})\n",
    "#print (soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revsion_txt (rvstart):\n",
    "    #https://fa.wikipedia.org/w/api.php?action=query&prop=revisions&titles=چنگال&rvlimit=10&rvslots=main&rvprop=content&rvdir=newer&rvstart=2016-07-01T00:00:00Z&rvexcludeuser=SSethiایران\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'prop': 'revisions',\n",
    "        'rvlimit':1,\n",
    "        'rvslots':'main',\n",
    "        'rvprop':'content|flags|ids|timestamp|user|comment|tags|size',\n",
    "        'rvdir':'newer',\n",
    "        'rvstart':rvstart\n",
    "    }\n",
    "    query = pywikibot.data.api.Request(site=fasite, **params).submit()\n",
    "    revision=query[u'query'][u'pages'][list(dict.keys())[0]][\"pageid\"]['revisions'][0]\n",
    "    flags=revision [\"flags\"]\n",
    "    revids=revision [\"revid\"]\n",
    "    users=revision [\"user\"]\n",
    "    timestamp=revision [\"timestamp\"]\n",
    "    size=revision [\"size\"]\n",
    "    comment=revision [\"comment\"]\n",
    "    tags=revision [\"tags\"]\n",
    "    txt=revision [\"content\"]\n",
    "    return [flags,revids,users,timestamp,size,comment,tags ,txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revsion_txt (26790691)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
